{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a54f08",
   "metadata": {},
   "source": [
    "# Ollama 설치 및 RAG 파이프라인 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b91ba8",
   "metadata": {},
   "source": [
    "## 3.2.2 Ollama 설치 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c70a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0aa09",
   "metadata": {},
   "source": [
    "## 3.2.3 Ollama를 사용한 Llama 모델 양자화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from ollama import quantize_model\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 모델 양자화\n",
    "quantized_model = quantize_model(model, bits=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616218a3",
   "metadata": {},
   "source": [
    "## 3.2.4 Ollama를 활용한 RAG 파이프라인 구성 및 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RAGChain\n",
    "from ollama import OllamaLLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 임베딩 모델 로드 및 문서 벡터화\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "data = [\"Ollama는 양자화를 통해 효율성을 높입니다.\", \n",
    "        \"RAG는 검색을 통해 정확한 응답을 제공합니다.\", \n",
    "        \"양자화는 실시간 상호작용에 유용합니다.\"]\n",
    "embeddings = [embedding_model.encode(doc) for doc in data]\n",
    "vector_store = FAISS.from_embeddings(embeddings, data)\n",
    "\n",
    "# Ollama LLM 설정\n",
    "llm = OllamaLLM(model_name=\"meta-llama/Llama-3.2-1B\")\n",
    "rag_chain = RAGChain(llm=llm, retriever=vector_store.as_retriever())\n",
    "\n",
    "# 질문 실행 및 결과 출력\n",
    "query = \"Ollama의 장점은 무엇인가요?\"\n",
    "result = rag_chain.run(query)\n",
    "print(\"Ollama RAG 응답:\", result)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
